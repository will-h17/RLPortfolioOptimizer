run_name: "ppo_improved"

seed: 42
paths:
  prices: "data/processed/prices_adj.parquet"
  features: "data/processed/features.parquet"
  out_root: "artifacts"

dates:
  train_end: "2019-12-31"
  val_end: "2020-06-30"
  test_end: "2021-03-31"

env:
  transaction_cost_bps: 0
  include_cash: true
  cash_rate_annual: 0.02
  normalize_obs_weights: true  # Enable observation normalization for better learning

reward:
  return_weight: 1.0
  turnover_penalty: 0.055642244227259365      # Small penalty to reduce excessive trading
  drawdown_penalty: 0.5983037979686836
  volatility_penalty: 0.28640410759202795

ppo:
  learning_rate: 4.316157612041365e-05         # Lower LR for more stable learning
  gamma: 0.9664577204636909                  # Higher discount factor (longer-term thinking)
  gae_lambda: 0.9011627554440242             # Higher GAE lambda (less bias in advantage estimates)
  n_steps: 4096                 # More steps per update (better gradient estimates)
  batch_size: 512               # Larger batches for more stable gradients
  clip_range: 0.29481917059030266
  ent_coef: 0.007649040082742959               # Higher entropy for more exploration
  policy_hidden_sizes: [256, 256, 128]  # Deeper network for more capacity

train:
  total_timesteps: 2000000      # 2M steps (4x more training)
  checkpoint_freq: 100000
  eval_freq: 50000

cost_model:
  enabled: true
  fee_bps: 0.0
  slippage_bps: 0.0
  spread:
    type: "vol20"
    fixed_bps: 2.0
    k_vol_to_bps: 4000.0        # Reduced from 8000.0 (less punitive during training)
    vol_col_suffix: "_s20"

